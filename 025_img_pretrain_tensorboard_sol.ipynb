{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard and Pretrained Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Some Data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "Tensorboard is a tool from Keras that can monitor the results of a tensorflow model and display it in a nice Tableau-like dashboard view. We can enable tensorboard and add it to our modelling process to get a better view of progress and save on some of the custom charting functions. \n",
    "\n",
    "The first thing that we can use tensorboard for is to get a nice chart of our training progress. \n",
    "\n",
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set # of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "pre = keras.metrics.Precision(name=\"precision\")\n",
    "rec = keras.metrics.Recall(name=\"recall\")\n",
    "metric_list = [acc, pre, rec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Tensorboard Callback\n",
    "\n",
    "The tensorboard can be added to the model as it is being fit as a callback. The primary parameter that matters there is the log_dir, where we can setup the folder to put the logs that the visualizations are made from. The example I have here is from the tensorflow documentation, generating a new subfolder for each execution. Using this to log the tensorboard data is fine, there's no need to change it without reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Tensorboard\n",
    "\n",
    "In recent versions of VS Code, whioch I assume all of you have, tensorboard can be used directly in a VS Code tab:\n",
    "\n",
    "![VS Code Tensor](images/vscode_tensorboard.png \"VS Code Tensor\" )\n",
    "\n",
    "The command below launches tensorboard elsewhere, such as Google colab.\n",
    "\n",
    "Either way, the actual tensorboard feature works the same once launched. We can open it before or after we start training the model. If we open it before we can update it to watch training progress - something that may be usefull if you have models that can train for a very long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-813f624e052e48c1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-813f624e052e48c1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit\n",
    "# The logdir is wherever the logs are, this is specified in the callback setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2195 - accuracy: 0.9344 - precision: 0.9548 - recall: 0.9161 - val_loss: 0.0990 - val_accuracy: 0.9692 - val_precision: 0.9756 - val_recall: 0.9656\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0968 - accuracy: 0.9700 - precision: 0.9748 - recall: 0.9666 - val_loss: 0.0777 - val_accuracy: 0.9736 - val_precision: 0.9774 - val_recall: 0.9705\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0697 - accuracy: 0.9782 - precision: 0.9815 - recall: 0.9755 - val_loss: 0.0720 - val_accuracy: 0.9780 - val_precision: 0.9805 - val_recall: 0.9754\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0526 - accuracy: 0.9832 - precision: 0.9852 - recall: 0.9814 - val_loss: 0.0730 - val_accuracy: 0.9788 - val_precision: 0.9807 - val_recall: 0.9772\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0429 - accuracy: 0.9862 - precision: 0.9874 - recall: 0.9851 - val_loss: 0.0799 - val_accuracy: 0.9775 - val_precision: 0.9795 - val_recall: 0.9762\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0375 - accuracy: 0.9873 - precision: 0.9886 - recall: 0.9864 - val_loss: 0.0663 - val_accuracy: 0.9794 - val_precision: 0.9816 - val_recall: 0.9781\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0306 - accuracy: 0.9898 - precision: 0.9904 - recall: 0.9892 - val_loss: 0.0721 - val_accuracy: 0.9803 - val_precision: 0.9814 - val_recall: 0.9793\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0277 - accuracy: 0.9906 - precision: 0.9914 - recall: 0.9900 - val_loss: 0.0686 - val_accuracy: 0.9812 - val_precision: 0.9824 - val_recall: 0.9799\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0255 - accuracy: 0.9916 - precision: 0.9920 - recall: 0.9912 - val_loss: 0.0650 - val_accuracy: 0.9835 - val_precision: 0.9843 - val_recall: 0.9826\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0224 - accuracy: 0.9926 - precision: 0.9932 - recall: 0.9922 - val_loss: 0.0863 - val_accuracy: 0.9802 - val_precision: 0.9811 - val_recall: 0.9799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a2f9a9e80>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=metric_list)\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=epochs, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Contents\n",
    "\n",
    "The first page of the tensorboard page gives us a nice pretty view of our training progress - this part should be quite straightforward. The board will capture whatever executions are in that log file, we can filter them on the side to see what we are currently working on, or use different log locations to keep things separate. \n",
    "\n",
    "Like the text results, we get whichever metrics were specified when setting up the model. \n",
    "\n",
    "#### Tensorboard Images\n",
    "\n",
    "We can also use the tensorboard to visualize other stuff. For example we can load up some images from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up a timestamped log directory.\n",
    "\n",
    "logdir = \"logs/train_data/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_writer.as_default():\n",
    "    # Don't forget to reshape.\n",
    "    images = np.reshape(x_train[0:25], (-1, 28, 28, 1))\n",
    "    tf.summary.image(\"25 training data examples\", images, max_outputs=25, step=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained Models\n",
    "\n",
    "As we've seen lately, training neural networks can take a really long time. Highly accurate models such as the ones that are used for image recognition in a self driving cars can take multiple computers days or weeks to train. With one laptop we don't really have the ability to get anywhere close to that. Is there any hope of getting anywhere near that accurate?\n",
    "\n",
    "We can use models that have been trained on large datasets and adapt them to our purposes. By doing this we can benefit from all of that other learning that is embedded into a model without going through a training process that would be impossible with our limited resources. \n",
    "\n",
    "We will look at using a pretrained model here, and at making modifications to it next time. \n",
    "\n",
    "#### Functional Models\n",
    "\n",
    "I have lied to you, I forgot that the pretrained models are not sequntial ones (generally, not as a rule), so some of the syntax here is for functional models. It leads to us using some slightly unfamiliar syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-28 17:15:13.191259: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)\n",
    "PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')\n",
    "\n",
    "train_dir = os.path.join(PATH, 'train')\n",
    "validation_dir = os.path.join(PATH, 'validation')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                            shuffle=True,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            image_size=IMG_SIZE)\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                 shuffle=True,\n",
    "                                                                 batch_size=BATCH_SIZE,\n",
    "                                                                 image_size=IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model\n",
    "\n",
    "There are several models that are pretrained and available to us to use. VGG16 is one developed to do image recognition, the name stands for \"Visual Geometry Group\" - a group of researchers at the University of Oxford who developed it, and ‘16’ implies that this architecture has 16 layers. The model got ~93% on the ImageNet test that we mentioned a couple of weeks ago. \n",
    "\n",
    "![VGG16](images/vgg16.png \"VGG16\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 160, 160, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 160, 160, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 80, 80, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 80, 80, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 80, 80, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 40, 40, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 40, 40, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 40, 40, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 40, 40, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 20, 20, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 20, 20, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 20, 20, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 20, 20, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 10, 10, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 10, 10, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12800)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 12801     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,727,489\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "input_tensor = Input(shape=(160, 160, 3))\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "63/63 [==============================] - 576s 9s/step - loss: 0.0000e+00 - accuracy: 1.0000 - precision: 0.2500 - recall: 0.0010 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 525s 8s/step - loss: 0.0000e+00 - accuracy: 1.0000 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 513s 8s/step - loss: 0.0000e+00 - accuracy: 1.0000 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f597c9760>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=metric_list)\n",
    "\n",
    "log_dir = \"logs/fit/VGG\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_dataset, \n",
    "          epochs=epochs, \n",
    "          validation_data=validation_dataset, \n",
    "          callbacks=[tensorboard_callback])\n",
    "          \n",
    "model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 176s 5s/step - loss: 0.0000e+00 - accuracy: 1.0000 - precision: 0.0000e+00 - recall: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Data\n",
    "\n",
    "We can use the rose data for a more complex dataset and a more interesting example in terms of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n",
      "['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import PIL \n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file(origin=dataset_url,\n",
    "                                   fname='flower_photos',\n",
    "                                   untar=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "#Flowers\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 64005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,778,693\n",
      "Trainable params: 64,005\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(180, 180, 3))\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(5)(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "92/92 [==============================] - 341s 4s/step - loss: 6.6734 - accuracy: 0.6999 - val_loss: 5.3380 - val_accuracy: 0.7820\n",
      "Epoch 2/2\n",
      "92/92 [==============================] - 330s 4s/step - loss: 1.1660 - accuracy: 0.9159 - val_loss: 4.9176 - val_accuracy: 0.8120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8a2f884670>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "            optimizer=\"adam\", \n",
    "            metrics=keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"))\n",
    "\n",
    "log_dir = \"logs/fit/VGG\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "callback = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True) \n",
    "\n",
    "model.fit(train_ds,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=val_ds,\n",
    "            callbacks=[tensorboard_callback, callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We can also utilize the tensorboard display to give us a view of hyperparameter tuning. This requires more work than a simple grid search, but the results are pretty similar. Below is an example adapted from the tensorflow docs. \n",
    "\n",
    "We'll do this with a simple model - a dense layer, a dropout, and the output, more complex ones are the same in their setup:\n",
    "<ol>\n",
    "<li> HP_NUM_UNITS - test a different number of units between 16 and 32. \n",
    "<li> HP_DROPOUT - the proportion of dropouts in the dropout. \n",
    "<li> HP_OPTIMIZER - we can try some different optimizers. \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 2us/step\n",
      "40960/29515 [=========================================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 3s 0us/step\n",
      "26435584/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n",
      "4431872/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load some data\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Parameters\n",
    "\n",
    "We can define the parameters we want to grid search here. Each one is one of these hParam objects - we assign it a name and a range of values to use, here we have a numerical and a discreet example. We list those variables in the hparams argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32, 48, 64]))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.4))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd', \"rmsprop\"]))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "  hp.hparams_config(\n",
    "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_OPTIMIZER],\n",
    "    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Test Models\n",
    "\n",
    "We can create our models inside of some helper functions - each one will run a model with certain HPs and return the accuracy, or whichever other metric we define. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "  ])\n",
    "  model.compile(\n",
    "      optimizer=hparams[HP_OPTIMIZER],\n",
    "      loss='sparse_categorical_crossentropy',\n",
    "      metrics=['accuracy'],\n",
    "  )\n",
    "\n",
    "  model.fit(x_train, y_train, epochs=10) \n",
    "  _, accuracy = model.evaluate(x_test, y_test)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "  with tf.summary.create_file_writer(run_dir).as_default():\n",
    "    hp.hparams(hparams)  # record the values used in this trial\n",
    "    accuracy = train_test_model(hparams)\n",
    "    tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfrom the GridSearch\n",
    "\n",
    "We have to write the gridsearch manually, but we can copy this basic setup as a template and modify it. Once complete, load tensorboard and go to the HPARAMS section to visualize. \n",
    "\n",
    "The parallel coordinates view allows us to do a quick exploration of the best HPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.7145 - accuracy: 0.7561\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5126 - accuracy: 0.8178\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4749 - accuracy: 0.8307\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4526 - accuracy: 0.8373\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4388 - accuracy: 0.8422\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4315 - accuracy: 0.8431\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4244 - accuracy: 0.8457\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4133 - accuracy: 0.8491\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4099 - accuracy: 0.8497\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4044 - accuracy: 0.8518\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4118 - accuracy: 0.8545\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 1.0497 - accuracy: 0.6378\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6937 - accuracy: 0.7610\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6176 - accuracy: 0.7865\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5769 - accuracy: 0.8011\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5503 - accuracy: 0.8094\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5337 - accuracy: 0.8147\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5200 - accuracy: 0.8188\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5093 - accuracy: 0.8222\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5026 - accuracy: 0.8242\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4930 - accuracy: 0.8283\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.8370\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.8246 - accuracy: 0.7077\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5880 - accuracy: 0.7904\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5489 - accuracy: 0.8034\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5263 - accuracy: 0.8086\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5158 - accuracy: 0.8106\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5023 - accuracy: 0.8166\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4947 - accuracy: 0.8187\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4900 - accuracy: 0.8189\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4844 - accuracy: 0.8225\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.4774 - accuracy: 0.8242\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8480\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 1.1057 - accuracy: 0.5915\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.7863 - accuracy: 0.7190\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.7029 - accuracy: 0.7489\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.6574 - accuracy: 0.7647\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6242 - accuracy: 0.7771\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.6052 - accuracy: 0.7841\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5857 - accuracy: 0.7929\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.5764 - accuracy: 0.7955\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5678 - accuracy: 0.7980\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5616 - accuracy: 0.8001\n",
      "313/313 [==============================] - 0s 859us/step - loss: 0.4648 - accuracy: 0.8318\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'adam'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.5919 - accuracy: 0.7932\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4334 - accuracy: 0.8458\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3997 - accuracy: 0.8564\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3814 - accuracy: 0.8615\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3664 - accuracy: 0.8666\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3569 - accuracy: 0.8701\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3478 - accuracy: 0.8732\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3399 - accuracy: 0.8756\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3337 - accuracy: 0.8774\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3274 - accuracy: 0.8790\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.3649 - accuracy: 0.8666\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'dropout': 0.1, 'optimizer': 'sgd'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.9340 - accuracy: 0.6872\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6104 - accuracy: 0.7923\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5450 - accuracy: 0.8128\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5095 - accuracy: 0.8220\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4870 - accuracy: 0.8308\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4691 - accuracy: 0.8363\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4566 - accuracy: 0.8389\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4459 - accuracy: 0.8414\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4362 - accuracy: 0.8455\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 968us/step - loss: 0.4287 - accuracy: 0.8492\n",
      "313/313 [==============================] - 0s 868us/step - loss: 0.4319 - accuracy: 0.8470\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'adam'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6539 - accuracy: 0.7738\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4798 - accuracy: 0.8275\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4460 - accuracy: 0.8404\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4261 - accuracy: 0.8452\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4122 - accuracy: 0.8496\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4040 - accuracy: 0.8513\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3960 - accuracy: 0.8551\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3863 - accuracy: 0.8586\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3828 - accuracy: 0.8597\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3775 - accuracy: 0.8610\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8562\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'dropout': 0.2, 'optimizer': 'sgd'}\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.9441 - accuracy: 0.6763\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.6544 - accuracy: 0.7770\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5887 - accuracy: 0.7979\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5494 - accuracy: 0.8108\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 990us/step - loss: 0.5207 - accuracy: 0.8190\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.5030 - accuracy: 0.8244\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 977us/step - loss: 0.4862 - accuracy: 0.8303\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4773 - accuracy: 0.8344\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4656 - accuracy: 0.8371\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4580 - accuracy: 0.8383\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4324 - accuracy: 0.8458\n"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "  for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "    for optimizer in HP_OPTIMIZER.domain.values:\n",
    "      hparams = {\n",
    "          HP_NUM_UNITS: num_units,\n",
    "          HP_DROPOUT: dropout_rate,\n",
    "          HP_OPTIMIZER: optimizer,\n",
    "      }\n",
    "      run_name = \"run-%d\" % session_num\n",
    "      print('--- Starting trial: %s' % run_name)\n",
    "      print({h.name: hparams[h] for h in hparams})\n",
    "      run('logs/hparam_tuning/' + run_name, hparams)\n",
    "      session_num += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
